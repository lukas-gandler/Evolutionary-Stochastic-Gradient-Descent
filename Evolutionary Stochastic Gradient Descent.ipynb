{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f045d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from momentumnet import MomentumNet\n",
    "from momentumnet import transform_to_momentumnet\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky.backend.context import get_context\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import cv2\n",
    "\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "from smallnorb.dataset import SmallNORBDataset\n",
    "\n",
    "# Resolve some errors\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d6de9",
   "metadata": {},
   "source": [
    "# Define Datasets\n",
    "For the experiments with the Evolutionary Stochastic Gradient Descent algorithm we use the CIFAR10 and the smallNORB dataset. Since smallNORB is not available via torchvision we used python wrapper by ndrplz, more informations under https://github.com/ndrplz/small_norb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ab9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CIFAR10_loader(batch_size=100):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Pad(4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    # CIFAR-10 dataset\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='data/', train=True, transform=transform_train, download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='data/', train=False, transform=transform_test)\n",
    "\n",
    "    # Define DataLoaders\n",
    "    # use 'loky' to work with joblib\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4, multiprocessing_context=get_context('loky'))\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,   batch_size=batch_size, shuffle=False, num_workers=4, multiprocessing_context=get_context('loky'))\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def create_SmallNORB_dataset(batch_size=100):\n",
    "    dataset = SmallNORBDataset(dataset_root='data')\n",
    "    \n",
    "    # preprocess dataset\n",
    "    # -> convert from grayscale to RGB to get 3 channels\n",
    "    # -> move channel axis to the first position\n",
    "    # -> convert category from byte to long\n",
    "    train_dataset = []\n",
    "    for d in dataset.data['train']:\n",
    "        z = (np.array(np.moveaxis(cv2.cvtColor(d.image_lt, cv2.COLOR_GRAY2RGB), -1, 0), dtype=np.float32), np.array(d.category, dtype=np.int64))\n",
    "        train_dataset.append(z)\n",
    "    \n",
    "    test_dataset = []\n",
    "    for d in dataset.data['test']:\n",
    "        z = (np.array(np.moveaxis(cv2.cvtColor(d.image_lt, cv2.COLOR_GRAY2RGB), -1, 0), dtype=np.float32), np.array(d.category, dtype=np.int64))\n",
    "        test_dataset.append(z)\n",
    "    \n",
    "    # Define DataLoaders\n",
    "    # use 'loky' to work with joblib\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, multiprocessing_context=get_context('loky'))\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, num_workers=3, multiprocessing_context=get_context('loky'))\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19854d3b",
   "metadata": {},
   "source": [
    "# Res-Net\n",
    "We will now define the ResNets we are going to use for the experiments. The implementation of these networks was created by akamaster, more informations under https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d444bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "              \n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def resnet32(num_classes=10):\n",
    "    return ResNet(BasicBlock, [5, 5, 5], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def resnet44(num_classes=10):\n",
    "    return ResNet(BasicBlock, [7, 7, 7], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def resnet56(num_classes=10):\n",
    "    return ResNet(BasicBlock, [9, 9, 9], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def resnet110(num_classes=10):\n",
    "    return ResNet(BasicBlock, [18, 18, 18], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def resnet1202(num_classes=10):\n",
    "    return ResNet(BasicBlock, [200, 200, 200], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd013aa7",
   "metadata": {},
   "source": [
    "Additionally we will define a function to convert the ResNets above into momentum ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_momentum_resnet(resnet, use_bp=False, gam=0.9):\n",
    "    \"\"\"\n",
    "    A function to create a momentum resnet with randomly initialized weights.\n",
    "    \n",
    "    Parameters:\n",
    "    resnet: The network to be converted into a momentum ResNet\n",
    "    use_bp: If False it will use the reversible architecture to calculate gradients, else it will use normal backpropagation\n",
    "    gam:    Determines the gamma value in the momentum term.      \n",
    "    \"\"\"\n",
    "    \n",
    "    mresnet = transform_to_momentumnet(resnet,\n",
    "                                       [\"layer1\", \"layer2\", \"layer3\"], # The layers to make invertible\n",
    "                                       gamma=gam,                      # Momentum term of the momentum res-net\n",
    "                                       use_backprop=use_bp,            # False to have smaller memory footprint\n",
    "                                       is_residual=True)               # True, because forward rule is x + f(x)\n",
    "    \n",
    "    return mresnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60225ce7",
   "metadata": {},
   "source": [
    "Also define some auxilliary functions for saving, backups and printing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baea4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(network):\n",
    "    for param in network.parameters():\n",
    "        print(param.data)\n",
    "        \n",
    "    print(f\"-------------------\")\n",
    "    \n",
    "def save_to_file(network, name):\n",
    "    torch.save(network.state_dict(), f\"{name}.pth\")\n",
    "    \n",
    "def load_from_file(name):\n",
    "    resnet = create_momentum_resnet()\n",
    "    resnet.load_state_dict(torch.load(f\"backups/{name}.pth\"))\n",
    "    return resnet\n",
    "\n",
    "def create_backup(population, k):\n",
    "    \"\"\"\n",
    "    Creates backups of the population in the 'backups' directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    directory_exists = os.path.exists(\"backups/\")\n",
    "    if not directory_exists:\n",
    "        os.makedirs(\"backups/\")\n",
    "    \n",
    "    # create reminder of what generation we finished\n",
    "    filename = f\"backups/!finished_gen_{k}.txt\"\n",
    "    f = open(filename, 'w')\n",
    "    f.close()\n",
    "    \n",
    "    # save population\n",
    "    i = 0\n",
    "    for network in population:\n",
    "        save_to_file(network, f\"backups/backup_network_{i}\")\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5518d",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "The following cells will implement the SGD part of the Evolutionary Stochastic Gradient Descent algorithm. To make it more stable we added weight decay and gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_training(network, SGD_steps, lr, momentum, data_loader):\n",
    "    \"\"\" \n",
    "    Performs SGD training for the given network.\n",
    "    \n",
    "    In order to avoid overloading the GPU with too many networks to train, we will send the \n",
    "    network to the GPU for the training process, afterwards we will send it back to the CPU.\n",
    "    Also the optimizer and the criterion are defined here, so no need to define them earlier.\n",
    "    \n",
    "    Parameters:\n",
    "    network:     the network to be trained using SGD\n",
    "    SGD_steps:   the number of SGD steps we will perform\n",
    "    lr:          the base learning rate\n",
    "    momentum:    the momentum for SGD\n",
    "    data_loader: the data loader to load the training examples\n",
    "    \n",
    "    \"\"\"\n",
    "    network.to(device)\n",
    "    network.train()\n",
    "    \n",
    "    lr_multiplier = random.uniform(0.9, 1.1)\n",
    "    \n",
    "    # Create optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr_multiplier*lr, momentum=momentum, nesterov=True, weight_decay=0.0001)\n",
    "    \n",
    "    total_step = len(data_loader)\n",
    "    \n",
    "    for s in range(0, SGD_steps):        \n",
    "        for i, (images, labels) in enumerate(data_loader):    \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = network(images)            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            #Gradient Value Clipping\n",
    "            nn.utils.clip_grad_value_(network.parameters(), clip_value=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            #### print a debug message about status of training ####\n",
    "            #if (i+1) % 100 == 0:\n",
    "            #    print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(s+1, SGD_steps, i+1, total_step, loss.item()))\n",
    "        \n",
    "            # make some clean-up\n",
    "            del loss, outputs\n",
    "    \n",
    "    # move network back to cpu and return\n",
    "    network.cpu()\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cbc05",
   "metadata": {},
   "source": [
    "# Genetic Algorithm\n",
    "Now we will implement the Genetic Algorithms part. For Selection we chose Roulette-Wheel-Selection, Crossover is done through Average-Crossover and for Mutation we use a zero-meaned normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e06989",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crossover_and_mutation(parents, sigma=0.01):\n",
    "    \"\"\"\n",
    "    Performs crossover and mutation in order to create a new offspring.\n",
    "    \n",
    "    The offspring is created using the stated dicts of the parents. First we sum up the \n",
    "    weights and biases of the parents and then we take an average of them. We also add \n",
    "    noise in form of a Normaldistribution (mutation given as: N(0, sigma^2)).\n",
    "    \n",
    "    Parameters:\n",
    "    parents: The list of parents from which the offspring is created.\n",
    "    sigma:   The mutation strength of the normal distribution\n",
    "    \n",
    "    Returns: The newly created offspring\n",
    "    \"\"\"\n",
    "    \n",
    "    base_sd = parents[0].state_dict()\n",
    "    \n",
    "    # adjust keys to determine which layers to should be affected by crossover and mutation\n",
    "    # Example:\n",
    "    #keys = ['fc.weight', 'fc.bias']\n",
    "    keys = base_sd                    # use all layers to be affected\n",
    "    \n",
    "    # sum of the weights of the parent\n",
    "    for i in range(1, len(parents)):\n",
    "        # get state dict of other parent\n",
    "        parent_sd = parents[i].state_dict()\n",
    "        \n",
    "        # add parent weights to the base state dict\n",
    "        for key in keys:\n",
    "            base_sd[key] = base_sd[key] + parent_sd[key]\n",
    "            \n",
    "    \n",
    "    # average the sum\n",
    "    num_parents = len(parents)\n",
    "    for key in keys:\n",
    "        \n",
    "        # Create mutation\n",
    "        tensor_size = base_sd[key].size()\n",
    "        random_tensor = torch.normal(mean=0.0, std=sigma, size=tensor_size)\n",
    "        \n",
    "        # Average and add mutation\n",
    "        base_sd[key] = base_sd[key] / num_parents + random_tensor\n",
    "    \n",
    "    # create offspring\n",
    "    offspring = create_momentum_resnet()\n",
    "    offspring.load_state_dict(base_sd)\n",
    "    return offspring\n",
    "    \n",
    "\n",
    "def create_offspring(population, fitness, rho, sigma):\n",
    "    \"\"\"\n",
    "    Creates new offspring based on selection, crossover and mutation.\n",
    "    \n",
    "    Parameters:\n",
    "    population: The population of the generation\n",
    "    fitness:    The fitness evalutaions of the individuals in the population\n",
    "    rho:        How many parents we select for crossover\n",
    "    \n",
    "    Returns: A newly created offspring, produced by selection, crossover and mutation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform selection\n",
    "    parents = random.choices(population, weights=fitness, k=rho)\n",
    "    \n",
    "    # Perform crossover and mutation\n",
    "    offspring = crossover_and_mutation(parents, sigma)\n",
    "    return offspring\n",
    "\n",
    "\n",
    "def GA_training(population, pop_size, offspring_size, elitist_level, rho, sigma, data_loader):\n",
    "    \"\"\"\n",
    "    Performs GA to create a new population for the next generation\n",
    "    \n",
    "    Parameters:\n",
    "    population:     The population of the curret generation on which we will create new individuals\n",
    "    pop_size:       The population size of the whole generation\n",
    "    offspring_size: The total number of offsprings the GA-step will generate\n",
    "    elitist_level:  The percentage of elite individuals which get carried over to next generation\n",
    "    data_loader:    The data loader on which fitness will be evaluated \n",
    "    \n",
    "    Returns: The new population for the next generation\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Calculate fitness of trained population ###\n",
    "    # Parallel version:\n",
    "    #fitness = Parallel(n_jobs=2)(delayed(calc_loss)(population[i], data_loader) for i in range(pop_size))\n",
    "    \n",
    "    fitness = [calc_loss(population[i], data_loader) for i in range(pop_size)]\n",
    "    print(f\"--- -- Finished fitness evaluation, length: {len(fitness)}\")\n",
    "    \n",
    "    ### Create offspring population ###\n",
    "    fitness_weighted = [1/f for f in fitness]   # take inverse of loss so lower losses get higher fitness-values\n",
    "    offspring_population = [create_offspring(population, fitness_weighted, rho, sigma) for i in range(offspring_size)]\n",
    "    print(f\"--- -- Finished creating offspring population\")\n",
    "    \n",
    "    ### Evaluate fitness of offsprings ###\n",
    "    # Parallel version:\n",
    "    #offspring_fitness = Parallel(n_jobs=2)(delayed(calc_loss)(offspring_population[i], data_loader) for i in range(offspring_size))\n",
    "    \n",
    "    offspring_fitness = [calc_loss(offspring_population[i], data_loader) for i in range(offspring_size)]\n",
    "    print(f\"--- -- Finished evaluating fitness of offspring population\")\n",
    "    \n",
    "    # Combine fitness and population lists\n",
    "    combined_fitness = fitness + offspring_fitness\n",
    "    combined_population = population + offspring_population\n",
    "    \n",
    "    # sort population by their fitness values\n",
    "    sorted_population = [pop for _, pop in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    sorted_fitness = [loss for loss, _ in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    \n",
    "    # Select m-elitists from sorted population\n",
    "    m = int(pop_size * elitist_level)\n",
    "    new_population = sorted_population[0:m]\n",
    "    \n",
    "    # Fill up rest of population\n",
    "    difference = pop_size - m\n",
    "    remaining_population = list(set(sorted_population) - set(new_population))\n",
    "    filler_population = random.sample(remaining_population, difference)\n",
    "    \n",
    "    # assemble new population and return\n",
    "    new_population = new_population + filler_population\n",
    "    return new_population, sorted_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1601af4",
   "metadata": {},
   "source": [
    "Define some possible fitness evaluation functions. In our experiments the fitness was defined through the loss value on the training examples -> the lower the loss, the higher the fitness was. Another possibility would be to use the accuracy on the test examples. NOTE: The GA part above has to be adjusted when chaning the fitness functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test_model(network, data_loader):\n",
    "    \"\"\"\n",
    "    Tests the given network on the provided DataLoader.\n",
    "    \n",
    "    Parameters:\n",
    "    network:     The network to test \n",
    "    data_loader: The data to test on\n",
    "    \n",
    "    Returns: The accuracy of the network on the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # init accuracy\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    network.to(device)\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        #print('Accuracy of the model on the test images: {} %'.format(accuracy))\n",
    "        \n",
    "    # send network back to cpu\n",
    "    network.cpu()\n",
    "    return accuracy\n",
    "\n",
    "def calc_loss(network, data_loader):\n",
    "    \"\"\"\n",
    "    Calculates the loss of a network on the given DataLoader and returns the loss.\n",
    "    \n",
    "    Parameters:\n",
    "    network:     The network to test\n",
    "    data_loader: The data to test the network on\n",
    "    \n",
    "    Returns: The loss of the network in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    network.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = network(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += float(loss.item() * images.size(0))\n",
    "        del loss, outputs\n",
    "        \n",
    "    network.cpu()\n",
    "    return float(running_loss / len(data_loader.sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657950d",
   "metadata": {},
   "source": [
    "# Evolutionary Stochastic Gradient Descent\n",
    "The following cells will combine the SGD part with the GA part to create the hybrid genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14faa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESGD_algorithm(population, max_generations, SGD_steps, GA_steps, offspring_size, elitist_level, rho, base_lr, lr_milestones, train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Performs the Evolutionary Stochastic Gradient Descent algorithm for training neural networks.\n",
    "    \n",
    "    The hybrid genetic algorithm is a mixture between Genetic Algorithms (GA) and Stochastic Gradient\n",
    "    Descent (SGD). We first start off by initalizing our population of networks. Then we switch back\n",
    "    and forth between performing SGD and GA.\n",
    "    \n",
    "    Parameters:\n",
    "    population:     The initial populatoin\n",
    "    max_generatios: How many generations the whole algorithm should run\n",
    "    SGD_steps:      How many SGD steps we perform per generation\n",
    "    GA_steps:       How many GA steps we perform per generation (usually just one)\n",
    "    offspring_size: How many offsprings to generate during GA\n",
    "    elitist_level:  Number of elites getting a fix place in next generation\n",
    "    rho:            Number of parents to get draw for producing offspring (usually 2)\n",
    "    base_lr:        The base learning rate\n",
    "    lr_milestones:  The milestones on which the learning rate changes -> Usage: (GENERATION_NUMBER, NEW_LERANING_RATE)\n",
    "    train_loader:   Data Loader for loading training examples\n",
    "    test_loader:    Data Loader for loading test examples (for fitness evaluation)\n",
    "    \n",
    "    Returns: The trained population\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create initial population\n",
    "    pop_size = len(population)\n",
    "    print(f\"Starting with population of size: {pop_size}\")\n",
    "    \n",
    "    # create a checkpoint after before starting training to test backups\n",
    "    create_backup(population, 0)\n",
    "    \n",
    "    learning_rate = base_lr\n",
    "    \n",
    "    for k in range(max_generations):\n",
    "        print(f\"Currently in generation {k+1}\")\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        for (mile_k, mile_lr) in lr_milestones:\n",
    "            if mile_k == (k+1):\n",
    "                learning_rate = mile_lr\n",
    "                print(f\"Changed learning rate to {learning_rate}\")\n",
    "        \n",
    "        # Perform SGD\n",
    "        print(f\"--- Starting SGD\")\n",
    "        \n",
    "        # Parallel version\n",
    "        #population = Parallel(n_jobs=2)(delayed(SGD_training)(population[i], SGD_steps, learning_rate, 0.9, train_loader) for i in range(len(population)))\n",
    "        \n",
    "        # Sequential version\n",
    "        population = [SGD_training(population[i], SGD_steps, learning_rate, 0.9, train_loader) for i in range(pop_size)]\n",
    "        \n",
    "        print(f\"--- Finished SGD\")\n",
    "        \n",
    "        # create a checkpoint after SGD-steps\n",
    "        create_backup(population, k)\n",
    "            \n",
    "        # Perform GA\n",
    "        print(f\"--- Starting GA\")\n",
    "        sorted_fitness = []          # store the sorted fitness values to maybe use in data collection\n",
    "        for i in range(0, GA_steps):\n",
    "            sigma = 0.01 / (k+1)\n",
    "            population, sorted_fitness = GA_training(population, pop_size, offspring_size, elitist_level, rho, sigma, train_loader)\n",
    "        print(f\"--- Finished GA\")\n",
    "        \n",
    "        # create a checkpoint after generation finished\n",
    "        create_backup(population, k)\n",
    "        \n",
    "    print(f\"Finished training process\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab74ac",
   "metadata": {},
   "source": [
    "# Start training process\n",
    "We have now defined the whole training algorithm. The next step is to actually perform training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6c219",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define training dataset\n",
    "train_loader, test_loader = create_CIFAR10_loader(batch_size=64)\n",
    "#train_loader, test_loader = create_SmallNORB_dataset(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c8c7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create population and start training process\n",
    "population = [resnet20() for i in range(128)]\n",
    "#population = [create_momentum_resnet(resnet20()) for i in range(128)]\n",
    "\n",
    "trained_population = ESGD_algorithm(population=population,\n",
    "                                    max_generations=8,\n",
    "                                    SGD_steps=20,\n",
    "                                    GA_steps=1,\n",
    "                                    offspring_size=768,\n",
    "                                    elitist_level=0.6,\n",
    "                                    rho=2,\n",
    "                                    base_lr=0.1,\n",
    "                                    lr_milestones = [(4, 0.01), (6, 0.001)],\n",
    "                                    train_loader=train_loader,\n",
    "                                    test_loader=test_loader\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c6432",
   "metadata": {},
   "source": [
    "# Memory Consumption\n",
    "In this section we now want to measure the memory requirements of both network verions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aec4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "#train_loader, _ = create_SmallNORB_dataset(batch_size=256)\n",
    "train_loader, _ = create_CIFAR10_loader(batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b39e1d",
   "metadata": {},
   "source": [
    "#### Memory requirements of regular ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1b155",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_networks = [create_momentum_resnet(resnet20(),   use_backprop=True, gamma=0.0),\n",
    "                create_momentum_resnet(resnet32(),   use_backprop=True, gamma=0.0),\n",
    "                create_momentum_resnet(resnet44(),   use_backprop=True, gamma=0.0),\n",
    "                create_momentum_resnet(resnet56(),   use_backprop=True, gamma=0.0),\n",
    "                create_momentum_resnet(resnet110(),  use_backprop=True, gamma=0.0),\n",
    "                create_momentum_resnet(resnet1202(), use_backprop=True, gamma=0.0)\n",
    "               ]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(net):\n",
    "    loss = criterion(net(images), labels)\n",
    "    loss.backward\n",
    "\n",
    "print(\"Starting memory measurments\")\n",
    "mem_res = []\n",
    "for network in res_networks:\n",
    "    (images, labels) = next(iter(train_loader))\n",
    "    used_mem_res = np.max(memory_usage((train, (network,))))\n",
    "    mem_res.append(used_mem_res)\n",
    "    \n",
    "print(mem_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16867bdd",
   "metadata": {},
   "source": [
    "#### Memory requirements of momentum ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fecd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_networks = [create_momentum_resnet(resnet20(),   use_backprop=False, gamma=0.9),\n",
    "                create_momentum_resnet(resnet32(),   use_backprop=False, gamma=0.9),\n",
    "                create_momentum_resnet(resnet44(),   use_backprop=False, gamma=0.9),\n",
    "                create_momentum_resnet(resnet56(),   use_backprop=False, gamma=0.9),\n",
    "                create_momentum_resnet(resnet110(),  use_backprop=False, gamma=0.9),\n",
    "                create_momentum_resnet(resnet1202(), use_backprop=False, gamma=0.9)\n",
    "               ]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(net):\n",
    "    loss = criterion(net(images), labels)\n",
    "    loss.backward\n",
    "    \n",
    "print(\"Starting memory measurments\")\n",
    "mem_mom = []\n",
    "for network in mom_networks:\n",
    "    (images, labels) = next(iter(train_loader))\n",
    "    used_mem_res = np.max(memory_usage((train, (network,))))\n",
    "    mem_mom.append(used_mem_res)\n",
    "    \n",
    "print(mem_mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "x_labels = [\"ResNet20\", \"ResNet32\", \"ResNet44\", \"ResNet56\", \"ResNet110\", \"ResNet1202\"]\n",
    "\n",
    "end   = len(x_labels)\n",
    "start = 0\n",
    "\n",
    "plt.plot(x_labels[start:end], mem_res[start:end], label=\"Regular ResNet\")\n",
    "plt.plot(x_labels[start:end], mem_mom[start:end], label=\"Momentum ResNets\")\n",
    "plt.ylabel(\"Memory (Gib)\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
